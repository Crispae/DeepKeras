{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1029d64e",
   "metadata": {},
   "source": [
    "## Anatomy of neural network\n",
    "---\n",
    "\n",
    "Training a neural network revolves around following object:  \n",
    "- *Layers*, which are combined into a network(or model)\n",
    "- The *input data* and corresponding targets\n",
    "- The *loss function*, which defines the feedback signal used for learning.\n",
    "- The *optimizer*, which detrmines how learning approaches. \n",
    "\n",
    "\n",
    "As shown below in the figure you can visulaize the networks, composed of layers that are chained together, maps the input data to predictions. The loss function then compares these predictions to the targets, producing a loss value: a measure of how well the networks's predictions match what was expected.The optimizer uses this loss value to update the networks's weights.\n",
    "\n",
    "<img src=\"3.jpeg\" height=\"400\" width=\"500\">\n",
    "\n",
    "**Let's take a closer look at layers, networks, loss function and optimizers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385b8aa",
   "metadata": {},
   "source": [
    "#### 1. Layers: the bulildings blocks of deep learning  \n",
    "___\n",
    "\n",
    "The fundamental data strucure in neural network is the layer, A layer is a data-processing module that takes as input one or more tensors and that outputs on or more tensors.Some layers are stateless, but more frequently layers have a state: the layer's weights, one or several tesnors learned from stochastic gradient descent, which together form the network's knowledge.  \n",
    "\n",
    "Different layers are appropriate for different types of data processing,like:\n",
    "- Simple vector data, stored in 2D Tensor of shape(samples,features) is often processed by *Densely connected layer* and also called *fully connected layer* or *Dense layer*.  \n",
    "\n",
    "- Sequence data, stored in 3D Tensors of shape(samples,timestamps,features), is processed by *recurrent layers* such as LSTM layer.\n",
    "- Image data,stored in 4D tensors, is usualluy processed by 2D convulations layers(Conv2D)\n",
    "\n",
    "Think layers as the LEGO bricks of deep learning, a metaphor that is made explicit by frameworks like keras.Building deeplearning models in keras is done by clipping together layers to form useful data-transformation pipeline.The *notation layer* compatiblity here refers specifically tot he fact that every layer will only accept input tensors of certain shapes and will return output tensors of certain shape.For example:  \n",
    "\n",
    "`from keras import layers\n",
    "layer = layers.Dense(32,input_shape(784,))`  \n",
    "\n",
    "Here we are creating a layer that only accepts as input 2D tensor where the first dimension is 784.This layer will return a tesnir where the first dimension has been transformed to be 32.  Thus this layer can only be connected to a downstream layer that expects 32-dimenisonal vectors as its input.Keras handle all of this things internally.   \n",
    "\n",
    "`from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32,input_shape=(784,)))\n",
    "model.add(layers.Dense((32,))`  \n",
    "\n",
    "- Here the second layer doesn't recieve an input shape argument- instead, it automatically inferred its input shape as being the output shape of the that came before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd023a",
   "metadata": {},
   "source": [
    "#### 2. Models: networks of layers\n",
    "---\n",
    "\n",
    "**A deep-learning model is a directed, acyclic graph of layers.**The most common instance is a linear stack of layers, mapping a single input into a single output.But there are wide variety of network topologies.Some common ones include:  \n",
    "- Two-branch networks\n",
    "- Multihead networks\n",
    "- Inception blocks  \n",
    "\n",
    "Toplogy of a network defines a `hypothesis plane`.We defined machine learninhg as \"searching for useful representation of some input data, within a predefined space of possiblites, using guidance from a feedback signal\".By choosing a network topology,we constrianed our `space of possbilites(hypothesis spaces)` to a specific series of tensor operations,mapping input data to output data.What we are searching here is a good set values for the weight tensors involved in these tensor opeartions.  \n",
    "\n",
    "Picking the right architecture is more an art than a science;Only practice can help ypu become a proper-neural-network architect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec913d",
   "metadata": {},
   "source": [
    "#### 3. Loss function and optimizers: Keys the configuring the learning process\n",
    "---\n",
    "\n",
    "Once the architecure of network is defined, we have to choose:\n",
    "- **Loss function (objective function)**: The quantity that will be minimized during training.It represents a measures for the task at hand.  \n",
    "\n",
    "- **Optimizer**: Determines how the network will be updated based on the loss function.It implements a specific variant of stochastic Gradient Descent(SGD)  \n",
    "\n",
    "A neural network that has multiple outputs may habe multiple loss function(one per output).But the gradient=descent process must be based on a single scalar loss value; so, for multiloss netwok, all losses are combined(Via averaging) into a single scalar quantity. \n",
    "\n",
    "Choosing the right objective function for the right problem is extremly important, your netwoek will take any shortcut it can to minimize the loss; so if objetive doesn't fully correleate with the succes for task in hand, network will end up doing things you may not have wanted.All neural networks will be just ruthless in lowering the loss function- so choose objective function wisely.  \n",
    "\n",
    "When it comes to common task, such as classification, regression and sequence predictiom, there are simple guidelines you can follow to choose the correct loss.\n",
    "- Binary cross entropy: two class classification\n",
    "- categorical cross entropy: many class classifcation\n",
    "- mean squared error: Regression problem.\n",
    "- connectionist temporal classification: sequence learning  \n",
    "\n",
    "Only when you are dealing with new problems, we have to develop our own objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09fc12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
