{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ff6aa0e",
   "metadata": {},
   "source": [
    "## The universal workflow of machine learning  \n",
    "\n",
    "The universal blueprint that can be used to attack and and solve any machine-learning problem, The blueprint ties together the concepts like problem defination,evaluation,feature engineering and fighting overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da7b32",
   "metadata": {},
   "source": [
    "#### 1. Defining the problem and assebling a dataset \n",
    "---\n",
    "\n",
    "First you must the problem at hand:\n",
    " - What will your input data be? what are you trying to predict? Machine can only learn to predict something if you have     availabel training data: For example, you can only learn to classify sentiments of movie reviews if you have both movie reviews and sentiments annotation availabel.As such, data availablity is the limiting factor at this stage.  \n",
    " \n",
    " - What type of problem are you facing? is it binary classification?Multiclass classifiction?scalar regression?Vector regression?Multiclass, multilabel classification?something else, like clustering,generation,or reinforcement learning?Identifying the problem type will guide your choice of model **architecture**, **loss function** and so on.\n",
    " \n",
    "#### Hypotheses at this stage:\n",
    "- Your output can be predicted given your outputs\n",
    "- Availabel dats is sufficiently informative to learn the realtionship between inputs and outputs.  \n",
    "\n",
    "\n",
    "Not all probelems can be solved; Just becuase you have assembeled examples of inputs X and target Y doesn't mean X contain enough information to predict Y.For instance, if you're training to predict the movements of a stock on the stock market given in recent price history, you are' unlikelu to succeed, because price history doesn;t contain much predictive information.\n",
    "\n",
    "One class of unsolvalbel probelems you should be aware of nonstationery probelms. Suppose uour;'re tryiing to build a recommedadtion system engine for clothinh, you're training it ine on month of data, and you wanted to start genrating recommedation of winter.One big issue is that kind the kind of clother people buy change from season to season: Clothes buying is a stationary phenomenon over the scale of few months.What you are trying to model changes over time,In this casse, the right move is to constatantly retian model on data from the recent past, or gather data from the recent past wehere data is sationary\n",
    ".For a cyclical problem like clothes like clothes buying, a few year's woth of data will suffice to capure seasonal variation- but rememeber to make time of the year an input of your model.  \n",
    "\n",
    "\n",
    "Keep in mind that machine learninhg can only be used to memorize patterns that are present in your training data. You can only recognize wht you've seen before using machine learning trianed on past data to predict the feature making the assumpation that future will behave like past.That often isn't the case.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad016a84",
   "metadata": {},
   "source": [
    "#### 2. Choosing a measure of suceess.\n",
    "---  \n",
    "\n",
    "\n",
    "To conrol something, you need to be abel to observe it.To achive success, you must define what do you men by success-accuracy? precision? recall?Customer-retantion rate? You metric of success will guide the choice of loss fucntion: What your model will optimize. It should directly align your higher-level goals, such as the success of your business.  \n",
    "\n",
    "- For balanced-classification problems, where every class is equally likely, accuracy and `area under the reciever operating characterstic curev`(ROC AUC) are common metrics.\n",
    "- For class-imbalanced problems, you can use precision and recall.\n",
    "- For ranking problems or mutilabel classification you can use mean average precision.  \n",
    "\n",
    "One can also define its own custom metric by which to measure success.These diverse metrics you can see in various kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ade46",
   "metadata": {},
   "source": [
    "#### 3. Deciding on an evaluation protocol  \n",
    "\n",
    "Once you know what you're aiming for, you must establish how you'll measure your current progress.The common evaluation protocol:\n",
    "1. Maintaining a hold-out validation set: The way to go whenyou have plenty of data.  \n",
    "2. Doing K-fold cross-validation: The right choice when you have too few samples for hold-out validation to be reliable.  \n",
    "3. Doing iterated K-fold validation: For perfroming highly accurate model evaluation o little data is available  \n",
    "\n",
    "In most cases, first one work fine in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3e96a",
   "metadata": {},
   "source": [
    "#### 4. Preparing your data\n",
    "---\n",
    "\n",
    "Once you know what you're training on, what you're optimizing for, and how to evaluate your appraoch, you are almost ready to begin training models. But first, you should format your data in a way that can be fed into a machine-learning model.Let's say for example in Deep neural networks:  \n",
    "- Data should be formatted in tensors.\n",
    "- The value taken by these tensors should be usually be scaled small values: for example in the `[-1,1][0,1]` range.\n",
    "- If different features take values in different ranges(Heterogenous data), then the data should be normalized.\n",
    "- You may want to do some feature engineering, especailly for small data problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7679948",
   "metadata": {},
   "source": [
    "#### 5. Developing a model that does better than baseline\n",
    "---  \n",
    "\n",
    "Your goal at this stage is to achieve statistical power,that is to develop a small model that is capable of beating a dumb baseline.  \n",
    "\n",
    "Note that it's not always possible to achieve statistical power.If you can't beat a random baseline after multiple resonable architecure, it may be the answer to the question you are asking isn't present in the input data. We make two hypothesis in the begging:\n",
    "- outputs can be predicted using input data\n",
    "- Available data has sufficent information to learn relationship between inputs and outputs  \n",
    "\n",
    "It may be the case the hyothesis is false, in this case go back to board and analyze the data.  \n",
    "\n",
    "Assuming that things go well, you need to make three key choices to build you first working model.  \n",
    "- *Last-layer activation*: This establish useful contraints on the network's output.For instances, the IMDB classification example used sigmoid in the last layer: the regression example didn;t use any last-layer activation and so on.  \n",
    "\n",
    "- *Loss- Function*: This should match the type of problem you're tring to solve.For example, the IMDB example used binary-crossentropy, the regression example uses \"mse\" and so on.  \n",
    "\n",
    "- *Optimization configuration*: what optimizers will you use? what will its learning rate be? In most cases. it's safe to go with 'rmsprop' and it's default learning rate.  \n",
    "\n",
    "Regarding the choice of a loss function, note that it isn't always possible to directly optimize for the metric that measures success on a problem. Sometimes there is no easy way to turn a metric into a loss function; loss function after all need to be compatible given only a minibatch data(ideally it is compatible for single value also) and must be differentiable(otherwise you can't use backpropogation to train your network).For instance widely used classification metric ROC AUC can't be directly optimized.Hence classification task, its common to optimize for a proxy metric of ROC AUC, such as crossentropy. In general, you can hope that the lower the crossentipy gets, the higher the ROC AUC value will go above.\n",
    "\n",
    "|               Problem type              \t| Last-layer activation \t| Loss function              \t|\n",
    "|:---------------------------------------:\t|-----------------------\t|----------------------------\t|\n",
    "| Binary classification                   \t| sigmoid               \t| binary_crossentropy        \t|\n",
    "| Mutliclass, single-label classification \t| softmax               \t| categotical_crossentropy   \t|\n",
    "| Multiclass, multilabel classification   \t| sigmoid               \t| binary_crossentropy        \t|\n",
    "| Regression to arbitrary values          \t| None                  \t| mse                        \t|\n",
    "| Regression to values between 0 and 1    \t| sigmoid               \t| mse or binary_crossentropy \t|  \n",
    "\n",
    "[Markdown table is generated from this website](https://www.tablesgenerator.com/markdown_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d4d1b",
   "metadata": {},
   "source": [
    "#### 6. Scaling up: Developing a model that overfits  \n",
    "---\n",
    "\n",
    "Once you've obtained a model that has a statstical poer, the question becomes,is your model sufficently powerful? Does it have enough layers and parameters to properly model the problem at hand?For instance a network with a single hidden layer with two units would have statiscial power on MNIST but wouldn;t be suffiecent to solve the problem well. Remember that universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting anf overfitting; between under capacity and over capacity.To figure out where this border lies you first you must cross it.  \n",
    "\n",
    "To figure out how big a model you'll need, ou must develop a model that overfits.This is failrly easy:  \n",
    "1. Add layer\n",
    "2. Makes the layers bigger.\n",
    "3. Train for more epochs.  \n",
    "\n",
    "Always montior the taining loss and validation losss, as elll as thr training and validation balues for nay metrics.When you see that the model's performance on the validation data begins to degrade, you've achieved overfitting.  \n",
    "\n",
    "The next stage is to start egularizing and tuning the model,  to get as close as possible to the ideal model that neither underfits nor overfits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc008736",
   "metadata": {},
   "source": [
    "#### 7. Regularizing your model and tuning your hyperparameters\n",
    "---  \n",
    "\n",
    "This is the most time taking steps, you'll repeatedly modify your model, train it, evaluate it on your validation data,modify it again, and repeat until the model is as good as it can get.These are things yo should try:  \n",
    "\n",
    "- Add dropout\n",
    "- Try different architecturs: add or remove layers\n",
    "- Add L1 and L2 regularization\n",
    "- Try different hyperparameters(such as number of units per layer or the learning rate of the optimizer) to find the optimal solution. \n",
    "- Optioanlly, iterate on feature engineering: add new features, or remove features that doesn't seems informative.  \n",
    "\n",
    "Be mindful of the following: every time you use feedback from your validation process to tune your model, you leak information about the validation process into the model,Repeated just a few times, this is innocous(not harmful); but done systemically over many iteration, it will eventually cause your model to overfit to the validation process.This makes evaluation process less reliable.  \n",
    "\n",
    "\n",
    "Once you've developed a statisfactory model configuration, you can train your final production model on all the available data(training and validation) and evaluate it on last time on the test set.If it turns out that performance on the test set is significantlly worse than the performance on validation data, this may mean either that your validation procedure wasn.t reliable after all, or that you began overfitting the validation data whil tuning the parametrs of the model.In this case, you may want to switch to a more reliable evaluation protocol(such as iterated k-fold validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6b647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
