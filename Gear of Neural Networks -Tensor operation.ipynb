{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78155a11",
   "metadata": {},
   "source": [
    "## The gears of neural networks: tensor operations\n",
    "---\n",
    "Much as any computer program can be ultimately reduced to a small set of binary operations on binary inputs(AND,NOR,OR and so on), all tansformation learned by deep neural network can be reduced to a handful of of *tensor operation* applied to tensors of numerical data. Different operation in Tensors are like Addition,Multiplication,division and so on.\n",
    "\n",
    "Let's say we are building our network by stacking dense layer on top of each other.Keras instance of dense layer looks like this.\n",
    "`Keras.layers.Dense(512,activation=\"relu\")`\n",
    "\n",
    "This layer can be considered as fuction, which takes ainput 2D tensor and return another 2D tensor- a new representation for the input tensor. Specifically, the function is as follows(where **W** is a 2D tensor and **b** is a vector, both are attribute of a layer):\n",
    "\n",
    "`output = relu(dot(W,input)+b)`\n",
    "\n",
    "Let's unpack this there are three tensor operations here:\n",
    "- A dot product (dot) between theinput tensor and a tensor name W.\n",
    "- An addition (+) between resulting 2D tensor and a vector b.\n",
    "- Then finally relu operation where max(x,0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36adad",
   "metadata": {},
   "source": [
    "#### Key attributes in Tensor\n",
    "\n",
    "A Tensor is defined by three key attributes:\n",
    "- `Number of axes(rank)` - A 3D tensor has thee axes, and a matrix has two axes.This is also called the tensor's `ndim` in pythin libraries such as numpy.  \n",
    "\n",
    "\n",
    "- `Shape` - This is a tuple of integers that describes how many dimensions the tensors has along each axis.For example let's say a matrix has hape (3,5) and the 3D tensor has shape (3,3,5). A vector has a shape with a single element, such as (5,), whereas a scalar has a an empty shape,().  \n",
    "\n",
    "\n",
    "- `Data type`- This is the type of the data contained int he the tensor, for instance a tensor's type could be float2,unit8,float64, and so son, On rare ocasions you may see 'char' tensor. Note that string tensor doesn't exist in numpy, because tenors  live in preallocated, contagiou memory segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e489aa",
   "metadata": {},
   "source": [
    "### 1. Element-wise operations\n",
    "---\n",
    "**Element-wise operations:** Operation that are applied independentaly to each entry in the tensor being considered.These operations are highly amneable to massevly parralel implemenatations. Naive python implementation of an element-wise operations can be written using the pythoon loop. Let's see an example.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41793cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12818d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## applying relu functiont to matrix using for loop.\n",
    "def naive_relu(x:input):\n",
    "    assert len(x.shape) == 2 ## it's a 2D numoy tensor.\n",
    "    \n",
    "    x = x.copy() ## make a copy of it for doing any operations toa void overwrite to input tensor.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] = max(x[i,j],0)## value below 0 are considered zero here.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b21d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.12102096, -0.70869042],\n",
       "        [-1.11280532,  0.33702367],\n",
       "        [ 1.64109751,  0.38418265],\n",
       "        [ 1.23431686, -0.38054473],\n",
       "        [ 0.58436407, -0.45413768]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_A = np.matrix(np.random.randn(5,2))\n",
    "matrix_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af0532c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6 µs ± 462 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "## Applying relu function using naive relu using for loop\n",
    "relu_res = naive_relu(matrix_A)\n",
    "relu_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b690a",
   "metadata": {},
   "source": [
    "In practice when using Numpy Array these opertaions  are available and optimized built-in numpy functions,Which themselves delegate the heavy lifting  to Basic Linear Algebra subprogrmas(BLAS).BLAS are low-level, highly parallel, effieicent tensor-manipulation routines that are typically iplemented in Fortan and C.So, in numpy these element wise operation are blazing fast.  \n",
    "\n",
    "**Let's see the time taken by numpy relu function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a680dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.64 µs ± 30.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "z  = np.maximum(matrix_A,0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ae6b7",
   "metadata": {},
   "source": [
    "Let's try naive element wise addition operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c227d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add(x,y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape ### X and Y are 2D Numpy Tensor\n",
    "    ## In matrix for addition and subtraction the shape and size of matrix has to be same.\n",
    "    \n",
    "    x = x.copy() ##Avoiding input Tensor.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] += y[i,j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a71ad65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 2],\n",
       "        [1, 2, 3],\n",
       "        [2, 3, 4],\n",
       "        [3, 4, 5],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_x = [[int(j+i) for j in range(3)]for i in range(5)]\n",
    "matrix_x = np.matrix(matrix_x)\n",
    "matrix_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b803f139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 2,  2,  2],\n",
       "        [ 2,  3,  4],\n",
       "        [ 2,  4,  6],\n",
       "        [ 2,  5,  8],\n",
       "        [ 2,  6, 10]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_y = [[int(j*i+2)for j in range(3)]for i in range(5)]\n",
    "matrix_y  =  np.matrix(matrix_y)\n",
    "matrix_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f91130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.9 µs ± 1.06 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "naive_add(matrix_x,matrix_y) ## time taken use naive method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7d2bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_result = naive_add(matrix_x,matrix_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc64af52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 2,  3,  4],\n",
       "        [ 3,  5,  7],\n",
       "        [ 4,  7, 10],\n",
       "        [ 5,  9, 13],\n",
       "        [ 6, 11, 16]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3745b",
   "metadata": {},
   "source": [
    "$$ x= \\begin{bmatrix}\n",
    "0&1&2\\\\\n",
    "1&2&3\\\\\n",
    "2&3&4\\\\\n",
    "3&4&5\\\\\n",
    "4&5&6\n",
    "\\end{bmatrix}  \n",
    "$$  \n",
    "\n",
    "\n",
    "$$ y= \\begin{bmatrix}\n",
    "2&2&2\\\\\n",
    "2&3&4\\\\\n",
    "2&4&6\\\\\n",
    "2&5&8\\\\\n",
    "2&6&10\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$x+y = \\begin{bmatrix}\n",
    "2&3&4\\\\\n",
    "3&5&7\\\\\n",
    "4&7&10\\\\\n",
    "5&9&13\\\\\n",
    "6&11&16\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8e44255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.07 µs ± 26.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "matrix_x + matrix_y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b9c935",
   "metadata": {},
   "source": [
    "As you can see the time taken by numpy module to calculate this is amazingly less as compare to the naive method.In the  backend all this method has c or fortan code which is wrapped by python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cd0a7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 2,  3,  4],\n",
       "        [ 3,  5,  7],\n",
       "        [ 4,  7, 10],\n",
       "        [ 5,  9, 13],\n",
       "        [ 6, 11, 16]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = matrix_x + matrix_y\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7847c7",
   "metadata": {},
   "source": [
    "### 2. Broadcasting\n",
    "---\n",
    "In our earlier naive implementation of `naive_add` only supports the addition of 2D tensor with indentical shapes.But in the dense layer, we add a 2D tensor with a vector.What happens with addition when the shapes of the two tensors beinf added is differ?  \n",
    "\n",
    "When possible, and if there's no ambigutiy, the smaller tensor will be broadcasted to match the shape of the larger tensor.Broadcasting conist of two steps.  \n",
    "1. Axes(called broadcasr axes) are added to the smaller tensor to match the ndin to natch the larger tensor.\n",
    "2. The smaller tensor is repeated alongside these new axes to match the full shape of large tensor.\n",
    "\n",
    "### Shape Compatibility Rules\n",
    "1. If x,y have a different number of dimensions, prepend **1's** to the shape of the shorter.\n",
    "2. Any axis of length 1 can be repeated(broadcast)to the length of other vector's length in that axis.\n",
    "3. All other axes must have matching lengths.  \n",
    "\n",
    "`x,shape == (2,3)\n",
    "y.shape == (2,3) # compatible\n",
    "y.shape == (2,1) # compatible\n",
    "y.shape == (1,3) ## compatible as we can prepend 1 to match dim\n",
    "y.shape == (3,) ## here also prepend 1\n",
    "#result in (2,3) shape\n",
    "y.shape == (3,2) #not compatibe\n",
    "y.shape == (2,) ## cannot append 1 later`\n",
    "\n",
    "Let's take a concrete example. conder x with shape(32,10) and y with the shape (10,).First we add an empty first axis to y,ehose shape becomes (1,10). Then, we repeat y 32 times alongside this new axis, so that we end up with the tensor Y with the shape (32,10), Where `Y[i,:] == y` at this pint we can proceed with X and Y, because they have same shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "717bef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x,y):\n",
    "    assert len(x.shape) == 2 ## numpy tensor\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy() ## avoid overwriting\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] += y[j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "227a7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_A = np.matrix(np.random.random((3,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72923361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.22507737, 0.41597762, 0.16978782, 0.78754142, 0.24128524],\n",
       "        [0.65142811, 0.58775953, 0.04436466, 0.44562289, 0.32671138],\n",
       "        [0.42489012, 0.29793036, 0.1955798 , 0.47047185, 0.71442754]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aea1dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_B = np.random.random((5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d49e1eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09059811, 0.08373514, 0.06392869, 0.93048703, 0.25728368])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9c84782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.31567548, 0.49971275, 0.23371651, 1.71802845, 0.49856892],\n",
       "        [0.74202622, 0.67149467, 0.10829334, 1.37610992, 0.58399505],\n",
       "        [0.51548823, 0.3816655 , 0.25950849, 1.40095888, 0.97171122]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_add_matrix_and_vector(max_A,max_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8c15e",
   "metadata": {},
   "source": [
    "With broadcating, you can generally apply two-tensor element-wise operations if one tensor has shape (a,b,..n.n+1,...m) and other has shape (n,n+1,...m).The broadcasting will then automatically happen for axes a through n-1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106ef95",
   "metadata": {},
   "source": [
    "### 3. Tensor dot\n",
    "---\n",
    "The dot opeartion, aslo called a tensor product(not to be confused with element wise product) is the most common, most useful tensor opertation.Contrary to element-wise operations, it combines entries in the input tensor. \n",
    "\n",
    "\n",
    "An element-wise product is done with the * operator in numpy, keras,Theanso, and TensorFlow.`dot` uses a different suntaz in Tensorflow,but in Numpy and keras it's done using the standard dot operator.\n",
    "\n",
    "Vectors with only only same number of elements are allowed for dot product.  \n",
    "\n",
    "Dot product between matrix and vector can aslo be done , which return a vector where the coefficients are the dot product between vector and rows of Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94250150",
   "metadata": {},
   "source": [
    "##### 3.1 Vector Dot product\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "176efbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vector_dot(x,y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1 ## x and y are Numpy vectors.\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    Z = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        Z += x[i] * y[i] ##  multiply and then add them is dot, if we only multiply then then product.\n",
    "        \n",
    "    return Z       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "379ab25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector_A: [2 3] Vector_B:  [5 6]\n"
     ]
    }
   ],
   "source": [
    "Vector_A = (np.array((2,3)))\n",
    "Vector_B = (np.array((5,6)))\n",
    "print(\"Vector_A:\",Vector_A,\"Vector_B: \",Vector_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f57d95ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1 = naive_vector_dot(Vector_A,Vector_B)\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c967e00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vector_A @ Vector_B ## numpy dot product symbol is \"@\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af36b3",
   "metadata": {},
   "source": [
    "#### 3.2 matrix-vector dot product\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8df4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x,y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0] ## The columns of matrix must be equal to size of vector.\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):  ## \n",
    "        for j in range(x.shape[1]):\n",
    "            z[i]+= x[i,j] * y[j]\n",
    "            \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fbb3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_A = np.matrix([[2,3],[4,6],[5,11]])\n",
    "vector_B = np.array([3,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de2e28f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix_A: [[ 2  3]\n",
      " [ 4  6]\n",
      " [ 5 11]] vector_B:  [3 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"matrix_A:\",matrix_A,\"vector_B: \",vector_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "625a44e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24., 48., 81.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2 = naive_matrix_vector_dot(matrix_A,vector_B)\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c348fb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[24, 48, 81]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_A @ vector_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "14cdf4c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_346/3784928477.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatrix_B\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mvector_B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)"
     ]
    }
   ],
   "source": [
    "matrix_B @ vector_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d871be5",
   "metadata": {},
   "source": [
    "**NOTE: As soon as one of the two tesnor has an ndim greater than 1, dot is no longer symmetric, which is to say that dot(x,y) isn't the same as dot(x,y)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed7257",
   "metadata": {},
   "source": [
    "#### 3.3 Matrix-Matrix prodcut\n",
    "---\n",
    "**Important point for matrix dot product**  \n",
    "\n",
    "Dot product generalizes to tensors with an arbitary numbers of axes. The most common applcations may be the dot product between two matrices. The dot product between two matrices is only possible if `x.shape[1] == y.shape[0]` the result is a matrix with shape `(x.shape[0], y.shape[1])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eaa4da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x,y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len (y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0] ## The first dimension if x must be the same as oth dimension of y\n",
    "    \n",
    "    z = np.zeros((x.shape[0],y.shape[1]))\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = np.array(x[i,:])\n",
    "            print(row_x.)\n",
    "            column_y = np.array(y[:,j])\n",
    "            z[i,j] = naive_vector_dot(row_x,column_y)\n",
    "            \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dad72a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_A = np.matrix(([1,2],[4,5],[9,2]))\n",
    "matrix_B = np.matrix(([1,2,5],[4,5,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e1580cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 2],\n",
       "        [4, 5],\n",
       "        [9, 2]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "176b4b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 2, 5],\n",
       "        [4, 5, 9]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "022f4fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 9, 12, 23],\n",
       "        [24, 33, 65],\n",
       "        [17, 28, 63]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_A @ matrix_B ## gives 3 rows and 3 column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f1a6af",
   "metadata": {},
   "source": [
    "<img src=\"1.jpg\" width=\"300\" height=\"500\">  \n",
    "\n",
    "x,y,z are pictured as rectange.Because the rows and X and the columns of y must have the same size, it follows that the width of x must match the height of y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c03e68",
   "metadata": {},
   "source": [
    "#### 3.4 Tensor reshaping\n",
    "---\n",
    "Ii is not used in dense layer in neural network,it's main application is in preprocessing the data before feeding in network.  \n",
    "\n",
    "Reshaping a tensor means rearranging its rows and columns to match a traget shape.Naturally the reshaped tensor has the same total number of coefficients as the initial tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5a74c01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0,1],\n",
    "             [1,2],\n",
    "             [3,5]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "14ceff95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((6,1))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0df2a1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1],\n",
       "       [2, 3, 5]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((2,3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea608e5",
   "metadata": {},
   "source": [
    "**Transposing**  \n",
    "A special case of reshpaing that's commanly encountered is transposition.Transposing a matrix means exchanging its rows and its columns, so that `x[i,:]` changes to `x[:,i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b4f2497a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 300)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros((300,20))\n",
    "x = np.transpose(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17c2cf",
   "metadata": {},
   "source": [
    "#### 3.5 Geometric interpretation of tensor operations\n",
    "---  \n",
    "\n",
    "Because the contents of the tensors manipulated by tensosr operations can be interpreted as cordinated of points in some geometric space, all tensor opearations havea geometric interpretation.  \n",
    "\n",
    "In general,elementary geometric operations such as affine trnasformation, rotations,scaling, and so on can be expressed as tensor operations.\n",
    "\n",
    "\n",
    "#### 3.6 Geometric interpretation of deep learning\n",
    "---\n",
    "\n",
    "Neural networks consist entirely of chains of tensor operations and that all of these operation are just geometric trandformation of the input data. It follows that you can interpret a neural network as a very complex geometric trnasformations in a high-dimensional space, implementes via a long series of simple steps.  \n",
    "\n",
    "In 3D, Imagine two sheets of colured paper: one red and one blue.Put one on top of other. Now crumble them togther in a small ball. That crumbled paper ball is your input data, and each sheet of a paper is a class data like classification problem.What a neural network or any other machine-learning model will do is to figure out a transformation of the paper ball that would uncrumble, so as to make the two classes a clearly seprable again.With deep learning this would be implemnted as a series of simple transformation of the 3D space. Such as those which we can apply to the crumbled ball with our fingers, one movement at a time.  \n",
    "\n",
    "Uncrumbling paper balls is what machine learning is about: Finding neat represnetation for complex, highly folded data manifolds.Deep learning works so well because it take approach of incrementally decomposing a complicated geoemetric tranformation into a long chain of elementry ones, which os pretty much the strategy a human would follow to uncrumble a paper ball. Each layer in a deep network applies a transformation that disentangles the data a little-- and deep stack of layers makes tractable and extremly complicated disentanglement process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b6671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
