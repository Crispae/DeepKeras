{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c1f698",
   "metadata": {},
   "source": [
    "## Introduction üëãüëãüëã\n",
    "___\n",
    "\n",
    "We're about to dive into the theory of what convents are why they have been so successful at computer vision tasks.But first, let's take a pratical look at simple convnet example.It uses a convnet to classify MNIST digits,Even though convnet will be basic, it accuracy will blow out of the water that of the densely connected model.\n",
    "\n",
    "Convents architecure are basically stack of `Conv2D` and `MaxPooling2D` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad93a7",
   "metadata": {},
   "source": [
    "### Instantiating a small convnet\n",
    "___\n",
    "#### Dataset preparation üóÉÔ∏èüóÉÔ∏èüóÉÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65ccc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "913a0ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train),(x_test,y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f79d55e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dbfc657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e06f2077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "715daed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d576e",
   "metadata": {},
   "source": [
    "#### Normalizing train data  ‚úÇÔ∏è‚úÇÔ∏è‚úÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c6ccf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train /255\n",
    "x_test = x_test /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd90ab14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ee580dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a478c04",
   "metadata": {},
   "source": [
    "### label categorization\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "814b82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cc2e7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3607078",
   "metadata": {},
   "source": [
    "### Building convnet model üèóÔ∏èüèóÔ∏èüèóÔ∏è\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e30ec764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D\n",
    "from tensorflow.keras import models\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74c06175",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=(28,28,1),activation=\"relu\",))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbadf95",
   "metadata": {},
   "source": [
    "A convnet takes as input tensor of shape `(image_height,image_width,image_channels)`(not including batch dimension).In this case,we'll configure the convnet to process the input of size `(28,28,1)`, which is the format of MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab7b0c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b7ba3",
   "metadata": {},
   "source": [
    " As you can see that output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor od shape `(height,width,channels)`.The width and height dimensions tend to shrink as you go deeper in the network.The number of channels is controlled by the first argument passed to the `Conv2D` layers(32 or 64). \n",
    " \n",
    " The next step is to feed the last ouput tensor (of shape`(3,3,64)`) into a densely connected classifier network like those which we have implemented earlier: A stack of `Dense layers`.These classifiers process vectors, which are 1D, where the current ouput is a 3D tensor.First we have to flatten the 3D outputs to 1D, and then add a few `Dense` layers on top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac84be",
   "metadata": {},
   "source": [
    "### Adding a classifier on top of the convnet\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f8294fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8054c1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd0bf0",
   "metadata": {},
   "source": [
    "As you can see, the (3,3,64) outputs are flattend into vectors of shape(576,) before going through `two Dense Layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a64441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
